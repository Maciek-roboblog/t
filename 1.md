# LLaMA-Factory – Dostrajanie LLM na Kubernetes (GCP) z Argo i Artifactory

## Wprowadzenie: Czym jest LLaMA-Factory?

**LLaMA-Factory** to otwartoźródłowy zestaw narzędzi (autor: *hiyouga*), upraszczający trening i fine-tuning (dostrajanie) dużych modeli językowych (LLM) i modeli multimodalnych.  

Zapewnia:

- ujednolicony interfejs do dostrajania **100+ modeli** (rodziny: LLaMA, Mistral, ChatGLM, Qwen, itp.),
- obsługę wielu podejść treningowych:
  - SFT (Supervised Fine-Tuning),
  - metody alignment: PPO, DPO,
  - techniki redukcji VRAM: LoRA, QLoRA,
- standaryzację przebiegu fine-tuningu – ukrywa złożoność implementacji.

Dzięki temu:

- inżynierowie ML i mniej doświadczeni użytkownicy mogą **szybko zacząć** dostrajanie modeli na własnych danych,
- bez konieczności pisania kodu od zera.

Dostępne interfejsy:

- **CLI** – polecenia konsoli z konfiguracją w plikach YAML/JSON,
- **LLaMA Board** – webowe UI (Gradio) z formularzami i listami wyboru.

Dodatkowo:

- monitoring eksperymentów (TensorBoard, Weights & Biases, LLaMA Board),
- wsparcie dla deploymentu:
  - eksport adapterów LoRA,
  - API kompatybilne z OpenAI (backend **vLLM**).

Podsumowując: LLaMA-Factory skupia się na **wydajnym dostrajaniu modeli** i upraszcza cały proces – od danych, przez konfigurację i trening (zero-code w UI lub prosty YAML w CLI), aż po inferencję i wdrożenie modelu.

---

## Budowa obrazu Docker i wdrożenie LLaMA-Factory na Kubernetes (GKE)

### 1. Przygotowanie obrazu Docker

Ze względu na polityki bezpieczeństwa często nie można używać publicznych obrazów. Wtedy budujemy obraz samodzielnie.

Oficjalne repozytorium LLaMA-Factory udostępnia:

- Dockerfile dla różnych środowisk:
  - CUDA,
  - ROCm,
  - NPU,
- gotowe obrazy (np. `hiyouga/llamafactory:latest` – Ubuntu 22.04, CUDA 12.4, PyTorch 2.6).

Możemy:

1. Jako bazę użyć oficjalnego obrazu NVIDIA CUDA, np.:

   ```dockerfile
   FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04

2. Zainstalować wymagane biblioteki:

   * PyTorch (zgodny z CUDA),
   * transformers,
   * accelerate,
   * deepspeed,
   * `bitsandbytes` (dla QLoRA),
   * `vllm` (dla inferencji),
   * itd.

3. Zainstalować LLaMA-Factory, np.:

   ```dockerfile
   COPY . /app
   WORKDIR /app
   RUN pip install -e .[metrics]
   ```

Kluczowe: zapewnić zgodność wersji **CUDA** i **PyTorch** z wymaganiami LLaMA-Factory (obecnie PyTorch 2.4–2.6, CUDA 12.x).

Po przygotowaniu Dockerfile:

* budujemy obraz lokalnie (`docker build`) lub
* używamy **Cloud Build** na GCP. Przykładowy `cloudbuild.yaml`:

```yaml
steps:
  - name: 'gcr.io/cloud-builders/docker'
    args:
      [
        'build',
        '-t',
        '<REGION>-docker.pkg.dev/<PROJECT>/gke-llm/llama-factory:latest',
        '-f',
        'docker/docker-cuda/Dockerfile',
        '.'
      ]
images:
  - '<REGION>-docker.pkg.dev/<PROJECT>/gke-llm/llama-factory:latest'
```

Obraz trafia do prywatnego **Artifact Registry**, np.:

```text
<REGION>-docker.pkg.dev/<PROJECT>/gke-llm/llama-factory:latest
```

Spełnia to wymagania bezpieczeństwa: obraz jest w firmowym registry, zbudowany z zaufanych baz.

---

### 2. Klastry GKE i GPU

Do trenowania dużych modeli potrzebne są GPU. Tworzymy klaster GKE z node poolami GPU, np. z kartami **NVIDIA L4**.

#### Autoskalowanie

Warto włączyć autoscaling, np.:

* node pool `g2-standard-8` z 1×L4,
* node pool `g2-standard-24` z 2×L4,

oba z:

* `--min-nodes=0`
* `--max-nodes=3`
* `--enable-autoscaling`

Dzięki temu:

* w spoczynku – brak GPU (0 node’ów),
* przy pojawieniu się Jobów – węzły są automatycznie dodawane,
* po zakończeniu zadań – node’y są usuwane.

#### Sterowniki GPU

Podczas tworzenia puli:

* `--accelerator type=nvidia-...`
* `--gpu-driver-version=latest`

zapewnia obecność sterownika NVIDIA i runtime z obsługą GPU.
Alternatywa: ręczna instalacja NVIDIA Device Plugin DaemonSet.

Po utworzeniu klastra sprawdzamy:

```bash
kubectl get nodes
```

– węzły GPU pojawią się dopiero, gdy joby wystartują (autoscaler).

---

### 3. Wdrożenie podstawowej aplikacji

Dwa główne podejścia:

#### 3.1. Aplikacja stała (Deployment)

* Uruchamiamy kontener z UI (LLaMA Board).

* Tworzymy `Deployment` z naszym obrazem `llama-factory:latest`.

* Komenda startowa:

  ```yaml
  command: ["llamafactory-cli", "webui"]
  ```

* Domyślny port UI Gradio: **7860** – wystawiamy przez:

  * `Service` typu `LoadBalancer`,
  * lub `Ingress`.

Plusy:

* użytkownicy korzystają z UI przez przeglądarkę,
* mogą uruchamiać treningi z poziomu LLaMA Board.

Minusy:

* Gradio jest z natury aplikacją *jednoużytkownikową*,
* przy wielu użytkownikach/eksperymentach lepszy jest model batchowy.

#### 3.2. Zadania batchowe (Jobs)

Każdy trening to osobny **Job**. Przykład:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: llama-factory-train
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: trainer
          image: <REGISTRY>/llama-factory:latest
          args: ["llamafactory-cli", "train", "configs/my_model_lora.yaml"]
          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: "4"
              memory: "32Gi"
            requests:
              nvidia.com/gpu: 1
              cpu: "4"
              memory: "32Gi"
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: huggingface
                  key: HF_TOKEN
```

Tutaj:

* rezerwujemy **1 GPU** przez `nvidia.com/gpu`,
* autoscaler GKE doda node GPU, jeśli żaden nie jest dostępny.

#### 3.3. Podział na wiele replik / węzłów

Dla treningów multi-GPU / multi-node:

* LLaMA-Factory korzysta z **PyTorch Distributed (`torchrun`)**,
* w Kubernetes używamy m.in.:

**JobSet**:

* nowy kontroler pozwalający stworzyć grupę Jobów (np. 2 workery),
* każdy pod dostaje 1 GPU,
* wspólnie realizują trening modelu (np. 8B na 2 GPU),
* ustawiamy `MASTER_ADDR`, `MASTER_PORT` i uruchamiamy `torchrun`.

Alternatywy:

* **Argo Workflows** z wieloma podami,
* **MPI Operator**,
* własna logika (init-container, skrypty startowe).

Rezultat: można trenować duże modele, skalując na wiele maszyn; Kubernetes zarządza podami i siecią, a `torchrun` synchronizuje trening.

---

### 4. Skalowanie i autoskalowanie

Skalowanie dotyczy dwóch wymiarów:

#### 4.1. Skalowanie pojedynczego zadania

* Więcej GPU/CPU dla jednego fine-tuningu.
* LLaMA-Factory wspiera:

  * multi-GPU (Data Parallel),
  * offloading na CPU,
  * integrację z **DeepSpeed ZeRO**.
* Umożliwia trening modeli 70B+ przy wielu GPU lub z optymalizacją pamięci.

W Kubernetes:

* definiujemy odpowiednie `resources`,
* w razie potrzeby korzystamy z **multi-pod jobs**.

Jeśli potrzebujemy np. 8 GPU:

* możemy ustawić `nvidia.com/gpu: 8` na jednym podzie (node z 8 GPU),
* lub uruchomić 8 podów po 1 GPU i połączyć je w klaster treningowy (torchrun).

#### 4.2. Skalowanie wielu zadań (batchowo)

Scenariusz enterprise:

* wiele modeli naraz (różne zespoły, eksperymenty),
* Kubernetes:

  * kolejkuje zadania,
  * z autoscalerem dodaje node’y GPU na żądanie.

Przykład:

* node pool GPU z `min=0`, `max=3`,
* przy 3 równoległych Jobach – 3 maszyny GPU,
* kolejne Joby czekają, aż node’y się zwolnią lub zostaną dodane (o ile `max-nodes` na to pozwala),
* często tworzy się osobną pulę GPU **spot/preemptible** dla obniżenia kosztów.

Efekt: platforma enterprise, która:

* trenuje wiele modeli równolegle,
* lub jeden model na wielu GPU,
* dynamicznie dostosowuje zasoby,
* minimalizuje koszty (0 node’ów gdy brak zadań).

---

## Korzystanie z LLaMA-Factory: UI vs YAML/CLI

LLaMA-Factory jest elastyczne: pozwala używać **YAML + CLI** lub **LLaMA Board (UI)**.

### 1. Konfiguracje YAML i CLI

Plik YAML/JSON definiuje:

* `model` – nazwa/ścieżka modelu bazowego,
* `dataset` – nazwa/ścieżka zbioru, szablon formatowania dialogów,
* `method` – typ fine-tuningu:

  * `stage: sft`,
  * `finetuning_type: lora`, parametry LoRA itd.,
* sekcję `train` – batch size, learning rate, liczba epok,
* `output` – `output_dir`, logowanie, checkpointy itd.

Przykład uruchomienia lokalnego na 4 GPU:

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 FORCE_TORCHRUN=1 \
  llamafactory-cli train config.yaml
```

* `CUDA_VISIBLE_DEVICES` – wybór GPU,
* `FORCE_TORCHRUN=1` – wymusza użycie `torchrun` zamiast `python -m torch.distributed`.

Samo:

```bash
llamafactory-cli train config.yaml
```

* startuje trening zgodnie z parametrami w YAML,
* pobiera model bazowy i dane (np. z HF Hub),
* loguje przebieg na konsolę oraz do monitoringu (TensorBoard/W&B – jeśli skonfigurowano).

#### Integracja YAML w środowisku produkcyjnym

* pliki YAML trzymamy w repozytorium (wersjonowanie, dokumentacja),
* można je parametryzować,
* CLI pozwala **nadpisywać pola** z YAML:

  ```bash
  llamafactory-cli train my.yaml \
    learning_rate=5e-5 \
    num_train_epochs=4
  ```

Ważne: przy modelach takich jak LLaMA 2/3, trzeba mieć akceptację licencji i odpowiedni **token HF** (lub jego proxy w Artifactory – opis dalej).

---

### 2. Interfejs graficzny (LLaMA Board)

Uruchomienie:

```bash
llamafactory-cli webui
```

To startuje serwer Gradio (domyślnie `http://<host>:7860`).

W UI można:

* wybrać model z listy (w tym z ModelScope),
* wskazać zbiór danych (np. z `dataset_info.json` lub HF Datasets),
* ustawić metodę dostrajania (SFT/LoRA/QLoRA/PPO itd.),
* dobrać hiperparametry (batch size, lr, epoki),
* uruchomić **trening** lub **inferencję** przyciskiem.

Zalety:

* **no-code fine-tuning** – idealne do szybkich eksperymentów i dla nietechnicznych użytkowników,
* możliwość podglądu próbek danych i logów.

Warunki:

* UI musi działać na maszynie z GPU,
* w środowisku Kubernetes najprościej: 1 pod z GPU + `Service`.

---

### 3. Wyniki, eksport i testowanie

Wyniki z treningu:

* trafiają do `output_dir` (np. `saves/my-model/`),
* może to być:

  * pełny checkpoint,
  * adapter LoRA.

Eksport:

```bash
llamafactory-cli export config.yaml
```

* łączy wagi adaptera LoRA z modelem bazowym,
* zapisuje spójny model (`.bin`) zgodny z Hugging Face Transformers.

Testowanie modelu:

```bash
llamafactory-cli chat <ścieżka_config_lub_model>
```

* uruchamia prostą sesję czatu z nowym modelem.

---

## Automatyzacja potoku treningowego z Argo Workflows

**Argo Workflows** to silnik orkiestracji dla Kubernetesa – definiuje workflow jako serię kroków (kontenerów):

* wykonywanych sekwencyjnie lub równolegle,
* idealny do zrobienia **end-to-end pipeline** dla LLaMA-Factory.

Mamy dwa tryby:

* **bez Argo** – ręczne tworzenie Jobów/Deploymentów (omówione wyżej),
* **z Argo** – pełna automatyzacja.

Korzyści z Argo:

* jeden spójny przepływ:

  1. przygotowanie danych,
  2. pobranie modelu bazowego,
  3. trening,
  4. walidacja,
  5. rejestracja modelu,
  6. wdrożenie do API,
* każdy krok może mieć inny obraz kontenera,
* pełna integracja z Kubernetes:

  * zasoby per krok,
  * autoscaling node’ów,
  * przekazywanie artefaktów między krokami.

### Przykładowy workflow (wysoki poziom)

1. **Przygotowanie środowiska**

   * sprawdzenie danych/modeli,
   * tworzenie PV/PVC na dane/modele (jeśli potrzebne),
   * lekki obraz (np. `ubuntu`, `alpine`, `google/cloud-sdk`).

2. **Pobranie modelu bazowego z Artifactory**

   * jeśli nie chcemy łączyć się z HF,
   * krok pobiera pliki modelu (pytorch_model.bin, config.json, tokenizer, ...),
   * zapisuje je na wspólnym wolumenie (`/models/...`),
   * używa tokenu Artifactory (sekret w Kubernetes).

3. **Fine-tuning (trening)**

   * krok wykorzystuje obraz `llama-factory:latest`,

   * uruchamia:

     ```bash
     llamafactory-cli train path/to/config.yaml
     ```

   * konfiguracja może wskazywać model po ścieżce lokalnej:

     ```yaml
     model_name_or_path: /models/llama-2-7b
     ```

   * wyniki zapisywane do:

     * volume,
     * lub bucketu GCS (zamontowanego przez CSI/FUSE).

   * krok ma zdefiniowane wymagania GPU (np. przez `podSpecPatch` w Argo).

4. **Walidacja / Ewaluacja (opcjonalnie)**

   * uruchamia model na zbiorze walidacyjnym,
   * liczy metryki,
   * używa `llamafactory-cli evaluate ...` lub własnego skryptu.

5. **Eksport i wdrożenie**

   * eksport LoRA → pełny model,

   * zapis do Artifactory (registry modeli),

   * opcjonalne uruchomienie **serwisu inferencyjnego**:

     ```bash
     llamafactory-cli api config.yaml infer_backend=vllm
     ```

   * wystawia port (np. 8000) z API w stylu OpenAI,

   * pod można autoskalować (HPA + vLLM obsługuje wielu klientów).

Cały workflow:

* może być zarządzany przez Argo CD (GitOps),
* lub wywoływany ręcznie / z CRON.

### Obrazy kontenerowe

Potrzebne są:

* **główny obraz** `llama-factory:latest`:

  * trening, ewaluacja, eksport, ewentualnie serwowanie,
* **dodatkowe lekkie obrazy**:

  * do pobierania danych/modeli,
  * alternatywnie można użyć jednego cięższego obrazu do wszystkiego,
* (opcjonalnie) obraz serwisu wektorowego / Ray Serve, jeśli budujemy pełną aplikację RAG.

### Skalowanie w Argo

* Argo nie skaluje modeli, ale:

  * pozwala uruchamiać wiele instancji kroków równolegle (np. hyperparameter search – `matrix`, `parallelSteps`),
  * warto ustawić limity równoległości, by nie przeładować klastra,
  * autoskalowanie node’ów + monitoring zasobów (KEDA, HPA dla kontrolera Argo).

---

## Pobieranie modeli z Artifactory zamiast z Hugging Face

W środowisku korporacyjnym:

* internet/Hugging Face często jest ograniczony,
* firma utrzymuje **mirror w Artifactory** (np. JFrog),
* modele bazowe pobieramy z Artifactory, nie z `huggingface.co`.

Dwie główne strategie:

### A. Pre-download + lokalne ścieżki

1. Administrator (lub krok Argo) pobiera pliki modelu z Artifactory:

   ```text
   https://artifactory.local/huggingfaceml/LLMs/meta-llama/Llama-2-7b/...
   ```

2. Zapisuje do katalogu, np. `/data/models/Llama-2-7b`.

3. W YAML:

   ```yaml
   model_name_or_path: /data/models/Llama-2-7b
   trust_remote_code: true  # jeśli model tego wymaga
   ```

LLaMA-Factory:

* wykrywa ścieżkę lokalną,
* ładuje model z dysku,
* **nie** kontaktuje się z HF Hub.

W Argo:

* krok "pre-download" realizuje pobranie z Artifactory,
* krok treningu używa tego samego wolumenu.

### B. Artifactory jako proxy dla Hugging Face

**JFrog Artifactory**:

* może być skonfigurowany jako **Remote Repository** typu Hugging Face,
* cache’uje i proxuje zapytania.

Hugging Face Hub SDK respektuje zmienne środowiskowe:

* `HF_ENDPOINT`
* `HF_HUB_ETAG_TIMEOUT`
* `HF_HUB_DOWNLOAD_TIMEOUT`
* `HF_TOKEN`

Przykład konfiguracji:

```bash
export HF_HUB_ETAG_TIMEOUT=86400
export HF_HUB_DOWNLOAD_TIMEOUT=86400
export HF_ENDPOINT="https://company.jfrog.io/artifactory/api/huggingfaceml/huggingface-local"
export HF_TOKEN="<ARTIFACTORY_TOKEN>"
```

Skutek:

* `from_pretrained("meta-llama/Llama-2-7b")` idzie do Artifactory,
* Artifactory:

  * zwraca pliki z cache,
  * lub pobiera je z HF i cache’uje.

W Kubernetes:

* token (`HF_TOKEN`) przechowujemy w `Secret`,
* podajemy zmienne `env` w Pod/Workflow.

**Uwaga na tokeny:**

* `HF_TOKEN` tutaj to token Artifactory (autoryzacja do repo),
* nie token Hugging Face (choć nazwa zmiennej jest ta sama dla klienta).

Zalety podejścia B:

* brak zmian w logice LLaMA-Factory,
* w configu dalej podajemy np. `meta-llama/Llama-3-8B`,
* całość jest centralnie zarządzana w Artifactory.

### Przechowywanie wynikowych modeli

Po fine-tuningu:

* eksportujemy model (pełny checkpoint),
* zapisujemy do Artifactory / firmowego model hub,
* pipeline Argo może automatycznie:

  * wyeksportować model (`llamafactory-cli export ...`),
  * wysłać go do Artifactory.

Cała ścieżka:

1. model bazowy z Artifactory / proxy HF,
2. trening na Kubernetes,
3. zapis wytrenowanego modelu do Artifactory,

– odbywa się **wewnątrz infrastruktury korporacyjnej**.

---

## Podsumowanie

Wykorzystując **LLaMA-Factory na Kubernetes (np. GKE)** możemy zbudować skalowalną, enterprise’ową platformę do fine-tuningu LLM:

* przygotowujemy własny obraz Dockera i uruchamiamy go w klastrze GPU z autoskalowaniem,
* korzystamy z:

  * LLaMA Board (UI) do szybkich eksperymentów,
  * YAML + CLI do produkcyjnych pipeline’ów,
* **Argo Workflows** pozwala:

  * zdefiniować powtarzalny pipeline (dane → model bazowy → trening → ewaluacja → eksport → deployment),
  * integrować się z wewnętrznym **Artifactory** (modele bazowe i wynikowe),
* modele nie są pobierane z publicznych repozytoriów, tylko z korporacyjnego Artifactory (pre-download / proxy HF).

Tak zaprojektowana platforma:

* pozwala trenować **wiele modeli równolegle**,
* efektywnie wykorzystuje zasoby GPU,
* skaluje się automatycznie,
* jest zgodna z praktykami **MLOps** i politykami bezpieczeństwa w organizacji,
* korzysta z aktywnie rozwijanego LLaMA-Factory (wspiera najnowsze modele, np. Llama 3/4).
