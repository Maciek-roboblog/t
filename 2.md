Poniżej znajduje się **przeredagowana, uporządkowana i w pełni sformatowana wersja w Markdown** Twojego tekstu – bez zmian merytorycznych, jedynie z poprawioną strukturą, czytelnością i hierarchią nagłówków.

---

# Analiza organizacji i skalowania procesu fine-tuningu w LLaMA-Factory

## 1. Automatyczne skalowanie

### 1.1. Skalowanie pojedynczego procesu treningowego (wiele GPU i wiele węzłów)

Aby przyspieszyć trenowanie dużych modeli, jeden proces treningowy można uruchomić równolegle na wielu GPU, w tym na wielu węzłach. LLaMA-Factory wykorzystuje **PyTorch Distributed Data Parallel (DDP)**, dzięki czemu można używać `torchrun` lub DeepSpeed do rozpraszania obliczeń.

W Kubernetes powszechne jest uruchamianie **Kubernetes Job** z wieloma podami realizującymi wspólny trening.

#### Kluczowe mechanizmy:

* **Konfiguracja wielu podów**
  Tworzymy Job z `parallelism` równym liczbie podów. Niezbędny jest mechanizm rendezvous dla `torchrun` — zwykle realizowany przez **headless Service** (`clusterIP: None`).
  Przykład uruchomienia:

  ```yaml
  command: [
    "torchrun",
    "--nnodes", "2",
    "--nproc_per_node", "8",
    "--rdzv-backend", "c10d",
    "--rdzv-endpoint", "nanogpt-0.nanogpt-svc:29500",
    "train.py"
  ]
  ```

* **Zarządzanie jobem**

  * wszystkie pody muszą wystartować równocześnie,
  * warto cache’ować obraz Docker,
  * aby uniknąć "spóźnionych podów", stosuje się **gang-scheduling** (Volcano, Kueue).

* **JobSet lub operatory ML (Kubeflow)**
  JobSet automatycznie:

  * powiela szablony podów,
  * tworzy headless Service,
  * restartuje grupę podów w razie błędu.

* **Konfiguracja LLaMA-Factory**

  * CLI wspiera multi-GPU (`FORCE_TORCHRUN=1`),
  * konieczny NCCL,
  * LLaMA Board obsługuje tylko **single-GPU**, więc do rozproszonego treningu potrzebny jest CLI.

---

### 1.2. Skalowanie wielu równoległych zadań (kolejkowanie eksperymentów)

Aby uruchamiać wiele eksperymentów fine-tuningowych równolegle, potrzebne jest zarządzanie kolejkami oraz zasobami.

#### Narzędzia:

* **Argo Workflows**

  * każdy krok pipeline to uruchomienie jednego fine-tuningu,
  * możliwość równoległości (`parallelism`),
  * łatwe parametryzowanie YAML-i.

* **Kolejkowanie zasobów – Kueue / Volcano**

  * trzyma zadania w kolejce, dopóki nie ma GPU,
  * wspiera gang-scheduling.

* **Autoskalowanie klastra (GKE/EKS)**

  * automatyczne dodawanie node’ów GPU, gdy pojawiają się pending pods.

* **Monitoring**

  * status workflow w Argo,
  * monitoring kolejek w Kueue,
  * UI LLaMA-Board przy pojedynczych zadaniach.

---

## 2. Organizacja fine-tuningu na poziomie UI i YAML

### 2.1. Tworzenie i wersjonowanie plików konfiguracyjnych YAML

Pliki konfiguracyjne powinny być traktowane jako kod.

#### Najlepsze praktyki:

* **Repozytorium na konfiguracje**

  ```
  experiments/
    <nazwa_danych>/
      <model>/
        experiment1.yaml
        experiment2.yaml
  ```

* **Konwencje nazewnicze**

  * np. `ft_lora_lr1e-4_v1.yaml`,
  * nazwy powinny być jednoznaczne i opisowe.

* **Wersjonowanie w Git**

  * commit każdej zmiany configu,
  * możliwość tagowania release'ów.

* **Przykładowy YAML**

  ```yaml
  model_name_or_path: meta-llama/Llama-3.1-8B
  stage: sft
  finetuning_type: lora
  lora_rank: 8
  dataset: my_data
  template: llama3
  output_dir: saves/llama3-8b/lora/sft
  per_device_train_batch_size: 1
  num_train_epochs: 3.0
  learning_rate: 1e-4
  ```

---

### 2.2. Integracja LLaMA-Board z YAML (CLI vs UI)

* **Eksport konfiguracji z UI**
  UI powinien umożliwiać "Export config" (YAML/JSON), aby eksperyment był powtarzalny.

* **Import YAML do UI**

  * upload pliku,
  * lub wybór z repozytorium (idealnie: integracja z Git).

* **Spójność CLI/UI**

  * oba tryby powinny korzystać z tych samych formatów YAML,
  * UI dla prototypowania, CLI dla dużej skali.

* **Ograniczenia UI**

  * obsługuje tylko single GPU,
  * ale pozwala monitorować logi i metryki.

---

### 2.3. Nazewnictwo eksperymentów i organizacja wyników

* **Nazwy eksperymentów**
  Przykład:
  `wiki-llama2-7b-lora-rank8-lr1e4`

* **Struktura katalogów wynikowych**

  ```
  outputs/
    <dataset>/
      <model>/
        <experiment>/
          checkpoints/
          trainer_state.json
          pytorch_model.bin
          logs.txt
  ```

* **Unikanie nadpisywania**

  * własny katalog per run (`_v2`, data, timestamp).

* **Repozytorium vs wyniki**

  * configi w repo,
  * modele w storage,
  * w repo linki/metadata + README z mapą eksperymentów.

---

## 3. Przechowywanie artefaktów

### 3.1. Ścieżki docelowe i centralne magazynowanie

Artefakty są duże — wymagane jest centralne storage:

* **NFS/CephFS** lub
* **bucket w chmurze (GCS/S3)**

Przykład ścieżki:

```
gs://my-project-llm/experiments/<dataset>/<model>/<experiment>/
```

Dla GCS:

* można użyć **GCS FUSE CSI driver**,
* lub przesyłać pliki po zakończeniu joba (`gsutil cp`).

Możliwe także użycie **Artifactory / Model Registry**.

---

### 3.2. Zastosowanie zmiennych środowiskowych i wolumenów per zadanie

* **Dynamiczne ścieżki w Argo**

  ```yaml
  env:
    - name: OUTPUT_DIR
      value: "gs://my-project-llm/experiments/{{workflow.name}}"
  ```

* **Ephemeral PVC per job**

  * VolumeClaimTemplates,
  * upload wyników po zakończeniu workflow,
  * izolacja zadań.

* **Ustawienia w YAML (LLaMA-Factory)**
  `output_dir: /mnt/output`

* **Sekrety i dostęp (GCS/S3/Artifactory)**

  * Workload Identity,
  * Secret z kluczem,
  * zmienne środowiskowe z tokenami.

---

### 3.3. Rejestrowanie metadanych

* **Etykiety i adnotacje Kubernetes**

  * `experiment=name`, `owner=username`.

* **Trackery eksperymentów**

  * W&B, MLflow, TensorBoard,
  * automatyczny zapis configu, metryk, parametrów.

* **Własne logi eksperymentów**

  * CSV, Google Sheet, Notion,
  * automatyczne dopisywanie wpisów po jobie.

* **Nadawanie metadanych w nazwach**
  przykład: `20231120_jk_wiki_llama2_lora/`.

* **Dostęp do historii eksperymentów**

  * zakładka "History" w UI,
  * lub własna aplikacja odczytująca katalogi `experiments/` i `outputs/`.