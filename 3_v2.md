OK, zaktualizowałem poprzedni dokument architektury, uwzględniając to, co jest *naprawdę* w blogpoście Ricka Chena (GKE, L4, JobSet, Cloud Build, repo `llm-on-gke/LLaMA-Factory`).
Najważniejsze zmiany:

* doprecyzowanie **implementacji na GKE** (klaster, nodepoole L4, secret z HF tokenem),
* dodanie **JobSet** jako domyślnego mechanizmu dla trenowania rozproszonego (zgodnie z artykułem) – Argo zostaje jako opcjonalny orchestrator nad tym,
* doprecyzowanie **budowy obrazu** LLaMA-Factory (Cloud Build + Artifact Registry, gotowy `cloudbuild.yaml` z repo),
* lekkie doprecyzowanie GCS Fuse / storage artefaktów.

Poniżej poprawiona wersja dokumentu (pełny, samowystarczalny Markdown):

---

````markdown
# Architektura platformy fine-tuningu i inferencji LLM z LLaMA-Factory (C4)

## 1. Kontekst systemu (C4 – Poziom 1)

Platforma umożliwia zespołom Data Science trenowanie i udostępnianie własnych modeli **Large Language Model (LLM)** przy użyciu **LLaMA-Factory** w chmurze (np. Google Cloud, GKE).

System obsługuje dwa główne przypadki użycia:

1. **Fine-tuning modeli LLM** na danych użytkownika,
2. **Inference** – udostępnianie wytrenowanych modeli jako usługi API do wykorzystania w aplikacjach.

Główni użytkownicy:

- **Inżynierowie ML / Data Scientists** – przygotowują konfiguracje, uruchamiają treningi, analizują wyniki,
- **Klienci / aplikacje** – wywołują wdrożone modele przez API.

Założenia:

- środowisko **multi-tenant** – wiele zespołów korzysta ze wspólnej platformy,
- zgodność z **AI Act** – klasyfikacja modeli, dokumentacja danych i modeli, logowanie zdarzeń, audyt.

---

## 2. Architektura fine-tuningu (Wersja A – pipeline treningowy)

### 2.1. Diagram (C4 – poziom kontenerów)

Diagram pokazuje pipeline fine-tuningu z perspektywy jednego zespołu (Team A), przy założeniu:

- orkiestracja na Kubernetes – **Argo Workflows** (opcjonalnie),
- trenowanie rozproszone w ramach pojedynczego kroku – **Kubernetes JobSet** (jak w blogpoście GKE),
- zapisywanie artefaktów do **GCS**.

```plantuml
@startuml
!define ICONURL https://raw.githubusercontent.com/tupadr3/plantuml-icon-fontsprites/v2.1.0
!include ICONURL/common.puml
!include ICONURL/DevOps/helm.puml
!include ICONURL/AWS/cloud.puml // cloud icon as generic "cloud"

skinparam wrapWidth 200
skinparam defaultTextAlignment center

' Oznaczenie komponentów istotnych dla AI Act
skinparam rectangle<<AIAct>> {
  BackgroundColor #FFF2E5
  BorderColor #FF8040
  BorderThickness 2
}

actor "Data Scientist\n(zespół ML)" as Scientist <<Human>>

rectangle "Projekt GCP" <<cloud>> {
  node "Klaster Kubernetes (GKE)" as Cluster {
    package "Argo Workflows\n(Orkiestracja pipeline)\n[opcjonalnie]" as Argo <<framework>> {
      node "Argo UI/CLI" as ArgoUI <<framework>>
      node "Kontroler Argo\n(Workflow Controller)" as ArgoCtrl <<framework>>
    }

    frame "Namespace Team A" as NSA {
      component "Pipeline treningowy\n(Workflow Argo lub YAML K8s)" as Workflow
      component "JobSet: Fine-tuning LLM\n(workery LLaMA-Factory)" as TrainJobSet <<container>>
      component "Zadanie: Ewaluacja\n(Pod z LLaMA-Factory)" as EvalPod <<container>>
    }

    frame "Namespace Team B" as NSB {
      note "Inny zespół (multi-tenant)" as N2
    }

    node "Storage artefaktów\n(GCS bucket, GCS Fuse)" as ArtifactStore <<AIAct>>
    database "Repozytorium konfiguracji\n(Git – YAML/Helm)" as GitRepo
  }
}

Scientist -> ArgoUI : definiuje eksperyment\n(YAML config, dane)
ArgoUI -> ArgoCtrl : zgłasza uruchomienie\npipeline
ArgoCtrl -> Workflow : inicjuje Workflow (CRD)

' Możliwy wariant bez Argo:
Scientist --> Workflow : (alternatywnie)\n`kubectl apply -f fine-tune-jobset.yaml`

Workflow -> TrainJobSet : [krok] Fine-tuning (JobSet, multi-node)
TrainJobSet -> ArtifactStore : zapisuje wytrenowany model\n(checkpointy, finalny model)

Workflow -> EvalPod : [krok] Ewaluacja/wersja\nmodelu (opcjonalnie)
EvalPod -> ArtifactStore : zapisuje metryki, model card\n(opis modelu)

Workflow --> ArgoUI : status postępu / wyniki
@enduml
````

### 2.2. Opis komponentów (fine-tuning)

#### Data Scientist

* inicjuje proces trenowania:

  * poprzez Argo UI/CLI (workflow),
  * lub bezpośrednio `kubectl apply -f gke/fine-tune-*.yaml` (jak w blogpoście),
* przygotowuje:

  * konfigurację LLaMA-Factory (YAML),
  * wybór modelu bazowego (np. `meta-llama/Meta-Llama-3-8B-Instruct`),
  * dane (`dataset: identity, alpaca_en_demo`),
  * parametry LoRA/QLoRA.

#### UI/CLI (Argo lub „gołe” kubectl)

* **Argo UI / Argo CLI** (wariant bardziej enterprise / MLOps):

  * pozwala uruchomić i monitorować złożony workflow (preprocessing → fine-tune → ewaluacja → eksport → rejestracja),
* **kubectl bez Argo** (wariant z artykułu GKE):

  * bezpośrednie uruchamianie manifestów `JobSet` (`fine-tune-2x1-l4.yaml`, `fine-tune-2x2-l4.yaml`, `fine-tune-3x1-l4.yaml`),
  * prosta, ale nieco „ręczna” orkiestracja.

#### Argo Workflows (opcjonalny silnik orkiestracji)

* przydatny, gdy chcemy:

  * zautomatyzować cały pipeline end-to-end,
  * łączyć różne kroki (pobranie danych, trening, ewaluacja, eksport, rejestracja, deployment),
  * mieć historię workflowów, retry, parametryzację itp.
* w wariancie z artykułu GKE można potraktować Argo jako „nakładkę”:

  * pojedynczy krok Argo → uruchomienie **JobSet** (distributed training),
  * reszta pipeline’u (walidacja, eksport, publikacja) jako kolejne kroki.

#### JobSet (TrainJobSet – distributed fine-tuning)

* **JobSet** (kubernetes-sigs/jobset) to rekomendowany mechanizm w przykładzie GKE:

  * definiuje grupę zreplikowanych Jobów (np. `workers`),
  * każdy worker to Pod z obrazem LLaMA-Factory i 1 lub więcej GPU,
  * razem tworzą klaster PyTorch (`torchrun`) do treningu rozproszonego.

* Przykładowy JobSet (z artykułu):

  ```yaml
  apiVersion: jobset.x-k8s.io/v1alpha2
  kind: JobSet
  metadata:
    name: pytorch
  spec:
    replicatedJobs:
      - name: workers
        template:
          spec:
            parallelism: 2
            completions: 2
            template:
              spec:
                containers:
                  - name: pytorch
                    image: <REGION>-docker.pkg.dev/<PROJECT>/gke-llm/llama-factory:latest
                    env:
                      - name: MASTER_ADDR
                        value: "pytorch-workers-0-0.pytorch"
                      - name: MASTER_PORT
                        value: "3389"
                    command:
                      - bash
                      - -xc
                      - |
                        torchrun --nproc_per_node=1 \
                          --master_addr=$MASTER_ADDR \
                          --master_port=$MASTER_PORT \
                          train_llama_factory.py
  ```

* W repo **`llm-on-gke/LLaMA-Factory`** dostarczone są przykładowe pliki:

  * `gke/fine-tune-2x1-l4.yaml` – 2 nody z 1×L4,
  * `gke/fine-tune-2x2-l4.yaml` – 2 nody z 2×L4 (4 GPU),
  * `gke/fine-tune-3x1-l4.yaml` – 3 nody z 1×L4.

#### LLaMA-Factory (kontener treningowy)

* Obraz kontenera zawiera:

  * LLaMA-Factory (CLI + WebUI),
  * biblioteki: PyTorch, Accelerate, Bitsandbytes, DeepSpeed, vLLM, FlashAttention, W&B, itp.
* W trybie treningu:

  * pobiera model bazowy (z HF/Artifactory),
  * pobiera dane (`identity`, `alpaca_en_demo`, itp.),
  * uruchamia LoRA/QLoRA SFT z podanymi parametrami (`cutoff_len`, `max_samples`, `learning_rate` itd.),
  * zapisuje:

    * checkpointy,
    * finalne wagi (adapter lub pełny model),
    * logi metryk.

#### Repozytorium konfiguracji (Git)

* przechowuje:

  * pliki YAML LLaMA-Factory (`config.yaml`),
  * definicje `JobSet` (`gke/fine-tune-*.yaml`),
  * definicje workflowów Argo (jeśli używane),
* wspiera GitOps:

  * Argo CD może automatycznie deployować aktualne manifesty do klastra.

#### Storage artefaktów (GCS)

* **GCS bucket** pełni rolę:

  * docelowego storage’u modeli (`gs://<bucket>/models/...`),
  * miejsca na logi i metryki,
  * prostego „model registry” w wersji podstawowej.
* W przykładzie GKE:

  * nazwa bucketa ustawiana w JobSet (`bucketName: "mlops-repo"`),
  * GKE klaster ma włączony **GcsFuseCsiDriver**, więc można bucket montować jako wolumen.

### 2.3. Referencyjna implementacja na GKE (wg bloga)

1. **Klaster GKE**:

   * utworzony z małym nodepoolem CPU (np. `n2d-standard-4`) dla systemowych Podów,
   * dodane dwa spot nodepoole GPU:

     * `l4-node-pool` – 1×NVIDIA L4 / node,
     * `l4-2-node-pool` – 2×NVIDIA L4 / node,
   * oba z autoscalingiem `min-nodes=0`, `max-nodes=3`.

2. **Secret z tokenem Hugging Face**:

   ```bash
   export HF_TOKEN=<paste-your-own-token>
   kubectl create secret generic huggingface \
     --from-literal="HF_TOKEN=$HF_TOKEN"
   ```

3. **Budowa obrazu LLaMA-Factory** (repo `llm-on-gke/LLaMA-Factory` zawiera `cloudbuild.yaml` i `Dockerfile`):

   ```bash
   gcloud artifacts repositories create gke-llm \
     --repository-format=docker \
     --location=$REGION

   gcloud auth configure-docker $REGION-docker.pkg.dev

   gcloud builds submit . --region=$REGION
   ```

   Wynikowy obraz:
   `<REGION>-docker.pkg.dev/<PROJECT>/gke-llm/llama-factory:latest`

4. **Uruchamianie fine-tuningów**:

   * edycja `fine-tune-*.yaml`:

     * pole `image:` → własne `<REGION>-docker.pkg.dev/.../llama-factory:latest`,
     * pole `bucketName:` → istniejący bucket.
   * sekwencyjne uruchamianie JobSetów, np.:

     ```bash
     kubectl delete jobset pytorch --ignore-not-found
     kubectl apply -f gke/fine-tune-2x1-l4.yaml

     kubectl delete jobset pytorch
     kubectl apply -f gke/fine-tune-2x2-l4.yaml

     kubectl delete jobset pytorch
     kubectl apply -f gke/fine-tune-3x1-l4.yaml
     ```

---

## 3. Architektura rozszerzona o inference (Wersja B – serwowanie modeli)

Wersja B dodaje komponenty do **serwowania modelu jako API**:

* deployment modelu w trybie inference,
* autoryzację,
* monitoring i logowanie użycia,
* routing ruchu.

Diagram jest logiczny – nie zależy od tego, czy backend jest LLaMA-Factory+vLLM na GKE, czy inny serwer LLM.

```plantuml
@startuml
!define ICONURL https://raw.githubusercontent.com/tupadr3/plantuml-icon-fontsprites/v2.1.0
!include ICONURL/common.puml
!include ICONURL/DevOps/prometheus.puml

skinparam wrapWidth 200
skinparam defaultTextAlignment center

skinparam rectangle<<AIAct>> {
  BackgroundColor #FFF2E5
  BorderColor #FF8040
  BorderThickness 2
}

skinparam component<<new>> {
  BackgroundColor #D9E8FB
}

actor "Użytkownik\nAPI klient" as APIclient <<Human>>

rectangle "Klaster Kubernetes (Inferencja)" as InfCluster {
  component "Inference Service\n(Model LLM jako API)" as ModelAPI <<new>> {
    [Model Pod\n(LLaMA-Factory API/vLLM)] as ModelPod
  }
  component "API Gateway\n(Autoryzacja, Routing)" as APIGW <<new>>
  queue "Load Balancer\n/ Istio Ingress" as LB <<new>>
  component "Monitoring\nPrometheus, Grafana" as Monitoring <<new>><<AIAct>>
  component "Logi i audyty\n(Loki / GCP Logging)" as Logging <<AIAct>>
}

database "Registry modeli\n(metadane, kategorie)" as ModelRegistry <<AIAct>><<new>>
database "Karta modelu\n(Model Card repo)" as ModelCardRepo <<AIAct>><<new>>
storage "Artefakty modeli\n(GCS bucket / Artifactory)" as ModelStorage <<AIAct>>

Scientist2 <<Human>> as Scientist

Scientist --> ModelRegistry : rejestracja modelu\npo fine-tuningu
ModelAPI --> ModelStorage : ładuje wytrenowany\nmodel (wagi)
APIGW -> ModelAPI : zapytanie inferencyjne\n(tekst -> odpowiedź)
APIGW <- APIclient : zapytanie (prompt)
APIGW --> Monitoring : metryki (użycie, latencja)
ModelAPI -> Monitoring : metryki modelu (GPU, opóźnienia)
APIGW -> Logging : log żądania (kto, kiedy, parametry)
ModelAPI -> Logging : log szczegółowy (prompt, wynik)
@enduml
```
Jasne – poniżej dalsze sekcje, jako kontynuacja poprzedniego dokumentu (możesz je po prostu dokleić pod architekturą fine-tuningu i diagramem inference).


### 3.1. Nowe komponenty (inference)

- **Registry modeli (Model Registry)**  
  Wewnętrzna baza metadanych o wytrenowanych modelach. Po zakończeniu fine-tuningu zespół rejestruje model (ręcznie lub automatycznie w pipeline). Rejestr zawiera m.in.:

  - unikalny identyfikator / wersję modelu,
  - lokalizację artefaktów (GCS / Artifactory),
  - parametry treningu,
  - autorów,
  - kategorię ryzyka wg AI Act (high-risk / general-purpose),
  - odnośnik do karty modelu (Model Card).

  Wspiera wymóg prowadzenia rejestru systemów AI (audyt, compliance).

- **Karta modelu (Model Card)**  
  Ustrukturyzowany dokument opisujący:

  - przeznaczenie modelu,
  - ograniczenia,
  - wyniki testów,
  - znane biasy i ryzyka.

  Tworzona wg zaleceń Model Cards / System Cards – częściowo automatycznie po treningu (metryki, dane treningowe), a częściowo ręcznie przez zespół. Przechowywana w dedykowanym repo (ModelCardRepo – pliki Markdown/HTML, DB, system dokumentów).  
  Dla modeli wysokiego ryzyka publikacja karty modelu jest wymagana (transparentność).

- **Inference Service (Model API)**  
  Usługa odpowiedzialna za ładowanie modelu i obsługę zapytań inferencyjnych:

  - może korzystać z `llamafactory-cli api ...` z backendem **vLLM**,
  - wystawia API w stylu OpenAI (`/v1/chat/completions` itp.),
  - wdrażana jako `Deployment` lub `StatefulSet` w klastrze inferencyjnym.

  Przy starcie:

  - pobiera wagi z **ModelStorage** (GCS / Artifactory),
  - ładuje je do pamięci GPU/CPU,
  - obsługuje żądania od Gateway.

- **API Gateway**  
  Logiczny komponent pośredniczący między klientami a modelami:

  - autentykacja i autoryzacja (API key, OAuth2/JWT, mTLS),
  - kontrola dostępu do poszczególnych modeli / tenantów,
  - routing na podstawie hosta / ścieżki.  

  Może być zrealizowany jako:

  - Ingress (np. Istio Gateway / Envoy) z politykami bezpieczeństwa,
  - zarządzany gateway (np. Cloud Endpoints / Apigee na GCP),
  - własny microservice pełniący rolę bramki.

- **Load Balancer / Ingress**  
  Rozdziela ruch pomiędzy repliki serwisów modeli:

  - zapewnia high availability,
  - umożliwia horizontal scaling,
  - może być:
    - `Service` typu `LoadBalancer`,
    - częścią service mesh (Istio / Linkerd).

- **Monitoring (Prometheus, Grafana)** <<AIAct>>

  - zbiera metryki z:
    - API Gateway (liczba żądań, odpowiedzi 2xx/4xx/5xx, latencje),
    - serwerów modeli (zużycie GPU/CPU, czas generacji, długość odpowiedzi),
    - klastra jako całości (kube-state-metrics),
  - Grafana prezentuje dashboardy, alerty (SLA/SLO dla modeli).

  Monitoring jest istotny z punktu widzenia:

  - niezawodności i wydajności,
  - rozliczeń (koszt per model/tenant),
  - wymogów AI Act – ciągły nadzór nad działaniem modeli, wykrywanie odchyleń.

- **Logging i audyt (Loki / GCP Logging)** <<AIAct>>

  - centralizuje logi techniczne i audytowe,
  - API Gateway loguje:
    - kto (ID użytkownika/klienta),
    - kiedy (timestamp),
    - jaki model / endpoint,
    - wysokopoziomowe metadane żądania,
  - Model API loguje (w zależności od polityki prywatności):
    - prompt (lub jego zanonimizowaną/zhashowaną formę),
    - wygenerowaną odpowiedź,
    - kody błędów, ostrzeżenia.

  Logi:

  - przechowywane co najmniej przez minimalny czas wymagany przez AI Act (np. ≥6 miesięcy),
  - mogą być eksportowane do **BigQuery / Cloud Logging / archiwum** na potrzeby długoterminowego audytu,
  - pozwalają odtwarzać przebieg działania systemu w razie incydentu.

---

### 3.2. Przepływ (inferencja)

1. Po udanym fine-tuningu zespół rejestruje model w **Model Registry** (przez UI lub automatycznie w pipeline).
2. Przygotowywany jest manifest `Deployment`/`StatefulSet` dla **Model API**:
   - wskazanie obrazu (np. `llama-factory:latest`),
   - konfiguracja ścieżki do wag w **ModelStorage**,
   - definicja zasobów (GPU, CPU, RAM).
3. Argo CD / CI/CD wdraża serwis do klastra inferencyjnego.
4. Po starcie **Model API**:
   - ładuje wagi modelu z GCS/Artifactory,
   - zgłasza gotowość (`/healthz`, `readinessProbe`).
5. Klient (aplikacja backendowa, narzędzie, inny serwis) wysyła żądanie HTTP/gRPC do **API Gateway**.
6. **API Gateway**:
   - weryfikuje token / API key (autentykacja),
   - sprawdza uprawnienia (autoryzacja),
   - routuje żądanie do odpowiedniego endpointu modelu.
7. **Model API** generuje odpowiedź i odsyła ją przez Gateway do klienta.
8. W tle:
   - Prometheus zbiera metryki (Gateway, Model API, klaster),
   - Loki / Cloud Logging agreguje logi żądań, odpowiedzi i zdarzeń systemowych,
   - Dashboardy i alerty w Grafanie wspierają operacje i nadzór.

---

### 3.3. Skalowanie i routing dla inference

- **Skalowanie w poziomie (horizontal scaling)**:
  - zwiększenie liczby replik `Deployment`u modelu,
  - `HPA` (Horizontal Pod Autoscaler) może skalować na podstawie:
    - CPU/GPU,
    - liczby oczekujących żądań (metrics adapter),
    - metryk z Prometheusa (custom metrics).
  - Load Balancer/Ingress równoważy ruch między replikami.

- **Wiele modeli / wersji (routing)**:
  - **per subdomena**:  
    `model-a.team1.example.com` → Model A (team 1),  
    `model-b.team2.example.com` → Model B (team 2),
  - **per ścieżka URI**:  
    `/api/teamA/modelX/v1`, `/api/teamA/modelX/v2`, itd.
  - umożliwia:
    - równoległe utrzymywanie kilku wersji modelu (A/B testing, canary),
    - odseparowanie modeli różnych zespołów/klientów.

- **Blue-Green / Canary deployment**:
  - Argo CD (lub inne narzędzia GitOps) może wdrożyć nową wersję modelu obok starej,
  - API Gateway / Ingress:
    - najpierw kieruje mały procent ruchu na nową wersję,
    - po walidacji przełącza cały ruch,
  - w razie problemów łatwo zrobić rollback.

---

## 4. Warianty multitenancy i ich wpływ

Platforma jest zaprojektowana do pracy w trybie **multi-tenant** – wiele zespołów lub klientów korzysta z niej równolegle.

Rozważane są dwa główne warianty:

### 4.1. Logical tenancy – wspólny klaster, logiczna izolacja

W tym modelu:

- wszyscy tenantci korzystają z **jednego klastra Kubernetes**,
- izolacja jest zapewniona na poziomie konfiguracji i polityk.

Mechanizmy:

- **Namespaces per zespół / klient**  
  Każdy zespół posiada własny namespace, w którym:

  - uruchamia swoje JobSety / Joby treningowe,
  - hostuje swoje serwisy inference,
  - przechowuje Sekrety i ConfigMapy.

- **RBAC + ResourceQuota + LimitRange**

  - RBAC:
    - użytkownicy Team A mają dostęp tylko do namespace A, itd.,
  - `ResourceQuota`/`LimitRange`:
    - limity na CPU, pamięć i GPU per namespace,
    - zapobieganie monopolizowaniu zasobów przez jednego tenant’a.

- **Wspólne usługi platformowe**

  - Argo Workflows / Argo CD,
  - Prometheus, Grafana, Loki,
  - JobSet Controller, GCS Fuse CSI, itp.  
  Są zainstalowane raz (np. w `ops-system` namespace) i współdzielone.

- **Bezpieczeństwo**

  - konieczne są:
    - poprawne polityki RBAC,
    - NetworkPolicies izolujące ruch między namespace’ami,
    - polityki bezpieczeństwa (OPA/Gatekeeper, Pod Security),
  - wrażliwe dane (sekrety, dane treningowe) mogą być dodatkowo izolowane:
    - osobne buckety GCS per tenant + IAM,
    - osobne klucze KMS.

- **Zarządzanie i koszty**

  - uproszczona administracja – jeden klaster, jedna instalacja Argo, jeden stack monitoringu/logów,
  - lepsze wykorzystanie zasobów (często niższy koszt całkowity),
  - aktualizacje platformy dotyczą wszystkich tenantów naraz (wymagane dobre okna maintenance).

**Kiedy logical tenancy ma sens?**

- wewnętrzne zespoły w jednej organizacji,
- umiarkowane wymagania izolacyjne,
- silny nacisk na optymalizację kosztów i wspólny MLOps.

---

### 4.2. Hard tenancy – izolacja na poziomie infrastruktury

W tym modelu:

- każdy tenant (zespół / klient SaaS) ma **oddzielną infrastrukturę**:

  - własny klaster K8s (np. własny GKE),
  - potencjalnie własny projekt GCP / subskrypcję.

Konsekwencje:

- **Oddzielne klastry = mocna izolacja**

  - brak wspólnego API servera,
  - błędy konfiguracji w jednym klastrze nie wpływają na inne,
  - łatwiej spełnić ostre wymagania regulacyjne (bankowość, medycyna itd.).

- **Oddzielne komponenty platformowe**

  - każdy klaster ma własne:
    - Argo Workflows / Argo CD,
    - Prometheus, Grafana, Loki,
    - JobSet Controller, GCS Fuse CSI,
  - konfiguracje mogą być zarządzane centralnie (multi-cluster GitOps), ale wykonanie jest odseparowane.

- **Koszty i wykorzystanie zasobów**

  - większy narzut:
    - powielone usługi systemowe,
    - mniejsza elastyczność w dzieleniu zasobów GPU między tenantami,
  - uzasadnione, gdy:
    - klienci mają silne wymagania dot. izolacji,
    - SLA/kontrakty wymagają osobnych środowisk.

- **Zarządzanie**

  - rośnie złożoność operacyjna:
    - monitorowanie wielu klastrów (federowany Prometheus / centralna Grafana),
    - powtarzanie upgrade’ów w wielu środowiskach,
  - pomocne:
    - Argo CD w trybie multi-cluster,
    - GKE fleet / Anthos,
    - vCluster (wirtualne klastry we wspólnym fizycznym).

**Kiedy hard tenancy ma sens?**

- dla klientów zewnętrznych w modelu SaaS,
- dla środowisk o statusie „produkcyjnym krytycznym”,
- gdy wymagane są bardzo silne gwarancje izolacji.

---

## 5. Wymagania prawne (AI Act) – zgodność i oznaczenia

UE **AI Act** wprowadza wymogi dotyczące:

- klasyfikacji systemów,
- dokumentacji modeli i danych,
- logowania i nadzoru nad działaniem systemu.

Architektura platformy uwzględnia te wymagania poprzez:

### 5.1. Klasyfikacja modeli (risk classification)

- każdy model otrzymuje klasyfikację:

  - **High-Risk AI** (np. scoring kredytowy, rekrutacja, medycyna),
  - **General-Purpose AI** (ogólne LLM dla zastosowań niekrytycznych).

- informacje te są zapisywane w **Model Registry**,
- od kategorii ryzyka zależy:

  - zestaw wymaganych testów,
  - zakres logowania,
  - obowiązkowość Model Card / System Card,
  - konieczność przejścia procesu oceny zgodności.

### 5.2. Rejestr modeli (Model Registry) – dokumentacja techniczna

Model Registry pełni rolę wewnętrznego katalogu AI i części dokumentacji technicznej:

- przechowuje:

  - identyfikator modelu i wersję,
  - datę i autora utworzenia,
  - parametry treningu i użyte dane,
  - wyniki ewaluacji,
  - kategorię ryzyka,
  - linki do:
    - datasetów,
    - Model Card,
    - logów wdrożenia.

- umożliwia:

  - wykazanie, jakie systemy AI działają w organizacji,
  - szybkie przygotowanie informacji do audytu / organu nadzorczego,
  - powiązanie modeli z konkretnymi zbiorami danych i wersjami konfiguracji.

### 5.3. Dokumentacja datasetów (data provenance)

- pipeline wymusza:

  - dołączenie opisu zbioru danych (README/meta),
  - wskazanie źródła danych, zakresu, sposobu czyszczenia i anotacji,
  - opis znanych biasów / ograniczeń.

- te informacje są:

  - powiązane z modelem w **Model Registry**,
  - przechowywane razem z danymi (np. w osobnym repo danych).

- opcjonalnie:

  - użycie DVC / Git LFS do wersjonowania datasetów,
  - możliwość odtworzenia dokładnej wersji danych użytej do treningu (reproducibility).

### 5.4. Logowanie i audyt (record-keeping)

Komponent **Logging/Audyt** został zaprojektowany z myślą o wymaganiach AI Act dotyczących:

- rejestrowania zdarzeń,
- możliwości odtworzenia działania systemu.

Logowane są:

- wywołania inference (kto, kiedy, jaki model, status),
- decyzje/predykcje w systemach high-risk,
- operacje operacyjne (deployment, zmiany konfiguracji, eskalacje).

Logi są:

- przechowywane przez określony minimalny czas (np. ≥6 miesięcy, z marginesem bezpieczeństwa),
- zabezpieczone (dostęp tylko dla uprawnionych osób – audytorzy, SOC),
- dostępne do:

  - przeglądania on-demand,
  - generowania raportów compliance,
  - analizy incydentów.

### 5.5. Kontrola dostępu i bezpieczeństwo

Mechanizmy bezpieczeństwa na poziomie:

- Kubernetes (RBAC, Network Policies, Pod Security),
- chmury (IAM, KMS),
- API (API Gateway, OAuth2/JWT, mTLS),

służą m.in. do:

- ograniczenia dostępu do danych treningowych,
- ochrony endpointów inference (tylko uprawnieni użytkownicy),
- spełnienia zasad **human oversight** i odpowiedzialności.

### 5.6. Publikacja Model Card / System Card

Dla każdego wdrożonego modelu:

- generowana jest **Model Card** (i ewentualnie rozszerzona System Card),
- przechowywana w **ModelCardRepo**,
- może być:

  - udostępniana publicznie (dla modeli zewnętrznych),
  - udostępniana klientom (modele na zamówienie),
  - wykorzystywana wewnętrznie (dokumentacja techniczna).

Model Card zawiera:

- opis przeznaczenia,
- wyniki testów,
- ograniczenia i znane ryzyka,
- rekomendacje dotyczące odpowiedzialnego użycia.

### 5.7. Monitoring zgodności

Dzięki Monitoringowi:

- można wykrywać:

  - degradację jakości modeli,
  - anomalie w zachowaniu,
  - przekroczenia zadeklarowanych parametrów (SLA/SLO),

- w razie problemów:

  - zespół może zawiesić model,
  - uruchomić dodatkową ewaluację,
  - zaktualizować Model Card i dokumentację.

---

## 6. Helm charts – wykorzystywane repozytoria i komponenty

Implementacja platformy w Kubernetes wykorzystuje:

### 6.1. LLaMA-Factory

- brak oficjalnego Helm chartu,
- repo dostarcza **Dockerfile** i (w wariancie GKE) `cloudbuild.yaml`:

  - budujemy obraz (`llama-factory:latest`),
  - publikujemy do **Artifact Registry** (`<REGION>-docker.pkg.dev/<PROJECT>/gke-llm/llama-factory:latest`),
  - wykorzystujemy go w:
    - JobSetach treningowych,
    - Deploymentach inference (`llamafactory-cli api ...`).

- ewentualne własne charty:

  - `Chart.yaml` + `templates/deployment.yaml` dla API,
  - charty pipeline’ów treningowych (JobSet, Argo WorkflowTemplate).

### 6.2. Argo Workflows & Argo CD

- oficjalne repo: `argoproj/argo-helm` (GitHub),
- instalacja:

  ```bash
  helm repo add argo https://argoproj.github.io/argo-helm
  helm install argo-workflows argo/argo-workflows -n argo --create-namespace
  helm install argo-cd argo/argo-cd -n argocd --create-namespace


* charty dostarczają:

  * CRDy (`Workflow`, `WorkflowTemplate`, `Application`),
  * kontrolery,
  * serwery UI,
  * podstawową konfigurację RBAC.

### 6.3. GCS Fuse / CSI driver

* na GKE:

  * można włączyć dodatek `GcsFuseCsiDriver` przy tworzeniu klastra,
  * bucket GCS może być montowany jako `PersistentVolumeClaim` w Podach.

* alternatywnie:

  * użycie `csi-gcs` (open-source CSI driver),
  * helm charty dostępne na ArtifactHub (np. `softonic/csi-gcs`).

### 6.4. Monitoring – Prometheus, Grafana

* rekomendowany zestaw: `kube-prometheus-stack` (repo `prometheus-community`):

  ```bash
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm install monitoring prometheus-community/kube-prometheus-stack -n monitoring --create-namespace
  ```

* dostarcza:

  * Prometheus,
  * Alertmanager,
  * Grafana,
  * kube-state-metrics, node exporter,
  * przykładowe dashboardy.

### 6.5. Logging – Loki + Promtail

* repo: `grafana/helm-charts`,

* chart: `loki-stack`:

  ```bash
  helm repo add grafana https://grafana.github.io/helm-charts
  helm install loki grafana/loki-stack -n logging --create-namespace
  ```

* instaluje:

  * Loki (backend logów),
  * Promtail (daemonset na węzłach),
  * opcjonalnie Grafanę (jeśli nie używamy tej z kube-prometheus-stack).

* w produkcji warto:

  * przypiąć PersistentVolume do Loki,
  * rozważyć `boltdb-shipper` + GCS/S3 jako backend storage.

---

## 7. Źródła i inspiracje

* **LLaMA-Factory – GitHub & docs** – ogólna dokumentacja i przykłady fine-tuningu.
* **Distributed OpenSource LLM Fine-Tuning with LLaMA-Factory on GKE – Rick Chen** – referencyjna implementacja GKE + LLaMA-Factory + JobSet + L4.
* **Serving Open Source LLMs on GKE using vLLM** – wzorce serwowania modeli LLM na GKE.
* **Argo Workflows / Argo CD – oficjalne Helm charty i dokumentacja.**
* **Prometheus / Grafana / Loki – monitoring i logowanie w Kubernetes.**
* **EU AI Act, Model Cards, Label Studio / FairNow materiały dot. zgodności i dokumentacji modeli.**

```

Jeśli chcesz, mogę teraz złożyć z tego całość w **jeden spójny plik** (od tytułu do źródeł) albo zrobić krótszy „executive summary” na 1–2 strony.
```
