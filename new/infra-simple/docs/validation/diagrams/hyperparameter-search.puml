@startuml hyperparameter-search
!theme plain
skinparam backgroundColor #FEFEFE

title Hyperparameter Search Strategy

|Planning|
start
:Define Search Space;
note right
  **Parameters to tune:**
  * lora_rank: [8, 16, 32]
  * lora_alpha: [16, 32, 64]
  * learning_rate: [1e-4, 2e-4]

  **Total: 3 x 3 x 2 = 18 runs**
end note

:Set Budget Constraints;
note right
  * Max GPU hours: 72h
  * Max runs: 20
  * Early stopping: enabled
end note

:Run Baseline;
note right
  Default config:
  rank=8, alpha=16, lr=1e-4
end note

|Execution|
:Generate Experiment Grid;

fork
    :Launch Job: r=8, a=16;
fork again
    :Launch Job: r=8, a=32;
fork again
    :Launch Job: r=16, a=32;
fork again
    :Launch Job: r=16, a=64;
fork again
    :... more jobs;
end fork

:Wait for Completion;

while (Jobs running?) is (yes)
    :Monitor Progress in MLflow;
    :Check for Failed Jobs;

    if (Budget exceeded?) then (yes)
        :Cancel Remaining Jobs;
        break
    endif
endwhile (all done)

|Analysis|
:Collect Results from MLflow;

:Rank by eval_loss;

:Analyze Parameter Sensitivity;
note right
  **Results:**
  | rank | eval_loss |
  | 8    | 0.45      |
  | 16   | 0.38      |
  | 32   | 0.35      |

  Insight: Higher rank = lower loss
  but diminishing returns after 16
end note

:Identify Best Configuration;
#LightGreen:Best: rank=16, alpha=32, lr=2e-4;
note right: eval_loss = 0.36

|Decision|
if (Significant improvement\nvs baseline?) then (yes)
    :Document Findings;
    :Update Default Config;
    :Proceed to Final Training;
else (no)
    :Expand Search Space;
    :Try Different Parameters;
    :or Accept Baseline;
endif

stop

legend right
  **Search Strategies:**

  1. **Grid Search** (current)
     - Exhaustive
     - Good for small spaces

  2. **Random Search**
     - More efficient for large spaces
     - Better coverage

  3. **Bayesian Optimization**
     - Intelligent sampling
     - Requires more setup
endlegend

@enduml
