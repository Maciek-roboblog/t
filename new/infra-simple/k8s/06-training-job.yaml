# Job do treningu - uruchamiany ręcznie lub przez CI/CD
#
# ZALEŻNOŚCI:
# - MLflow (Secret: mlflow-config)
# - NFS Storage (PVC: llama-storage)
# - GPU Node (nvidia.com/gpu)
#
# WEJŚCIE:
# - Model bazowy: $BASE_MODEL_PATH (z PVC)
# - Dataset: $DATASET_PATH (z PVC)
#
# WYJŚCIE:
# - LoRA adapter: $LORA_OUTPUT_PATH (do PVC)
# - Metryki: MLflow
#
# Użycie: kubectl apply -f 06-training-job.yaml
#
apiVersion: batch/v1
kind: Job
metadata:
  name: llama-train
  namespace: llm-training
  labels:
    app: llama-train
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 86400
  template:
    metadata:
      labels:
        app: llama-train
    spec:
      restartPolicy: Never

      # GPU Node Selector
      nodeSelector:
        nvidia.com/gpu: "true"
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"

      containers:
      - name: trainer
        image: eu.gcr.io/PROJECT_ID/llama-factory-train:latest
        command: ["/bin/bash", "-c"]
        args:
        - |
          echo "=== LLaMA-Factory Training ==="
          echo "Model bazowy: ${BASE_MODEL_PATH}"
          echo "Output LoRA: ${LORA_OUTPUT_PATH}"
          echo "MLflow: ${MLFLOW_TRACKING_URI}"

          # Generuj config treningu z env vars
          cat > /tmp/train.yaml << EOF
          model_name_or_path: ${BASE_MODEL_PATH}
          stage: sft
          finetuning_type: ${FINETUNING_TYPE}
          lora_rank: ${LORA_RANK}
          lora_alpha: ${LORA_ALPHA}
          template: ${TEMPLATE}
          cutoff_len: ${CUTOFF_LEN}
          dataset_dir: ${DATASET_PATH}
          dataset: train_data
          output_dir: ${LORA_OUTPUT_PATH}
          per_device_train_batch_size: ${BATCH_SIZE}
          gradient_accumulation_steps: ${GRADIENT_ACCUMULATION}
          learning_rate: ${LEARNING_RATE}
          num_train_epochs: ${NUM_EPOCHS}
          fp16: true
          report_to: mlflow
          EOF

          # Uruchom trening
          llamafactory-cli train /tmp/train.yaml

          echo "=== Trening zakończony ==="
          ls -la ${LORA_OUTPUT_PATH}/

        # Konfiguracja z ConfigMap i Secret
        envFrom:
        - configMapRef:
            name: llm-config
        env:
        - name: MLFLOW_TRACKING_URI
          valueFrom:
            secretKeyRef:
              name: mlflow-config
              key: MLFLOW_TRACKING_URI

        # Storage
        volumeMounts:
        - name: storage
          mountPath: /storage

        resources:
          requests:
            cpu: "4"
            memory: "32Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "8"
            memory: "64Gi"
            nvidia.com/gpu: "1"

      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: llama-storage
