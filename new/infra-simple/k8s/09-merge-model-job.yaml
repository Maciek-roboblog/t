# Job do mergowania LoRA z modelem bazowym
#
# ZALEŻNOŚCI:
# - NFS Storage (PVC: llama-storage)
# - GPU Node (nvidia.com/gpu)
#
# WEJŚCIE:
# - Model bazowy: $BASE_MODEL_PATH (z PVC)
# - LoRA adapter: $LORA_OUTPUT_PATH (z PVC)
#
# WYJŚCIE:
# - Zmergowany model: $MERGED_MODEL_PATH (do PVC)
#
# Użycie: kubectl apply -f 09-merge-model-job.yaml
#
apiVersion: batch/v1
kind: Job
metadata:
  name: merge-lora
  namespace: llm-training
  labels:
    app: merge-lora
spec:
  backoffLimit: 1
  ttlSecondsAfterFinished: 3600
  template:
    metadata:
      labels:
        app: merge-lora
    spec:
      restartPolicy: OnFailure

      # GPU Node Selector
      nodeSelector:
        nvidia.com/gpu: "true"
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"

      containers:
      - name: merger
        # Używa tego samego obrazu co trening (ma LLaMA-Factory)
        image: eu.gcr.io/PROJECT_ID/llama-factory-train:latest
        command: ["/bin/bash", "-c"]
        args:
        - |
          echo "=== Merge LoRA ==="
          echo "Model bazowy: ${BASE_MODEL_PATH}"
          echo "LoRA adapter: ${LORA_OUTPUT_PATH}"
          echo "Output: ${MERGED_MODEL_PATH}"

          llamafactory-cli export \
            --model_name_or_path "${BASE_MODEL_PATH}" \
            --adapter_name_or_path "${LORA_OUTPUT_PATH}" \
            --template "${TEMPLATE}" \
            --finetuning_type lora \
            --export_dir "${MERGED_MODEL_PATH}" \
            --export_size 2 \
            --export_legacy_format false

          echo "=== Merge zakończony ==="
          ls -la "${MERGED_MODEL_PATH}/"

        # Konfiguracja z ConfigMap
        envFrom:
        - configMapRef:
            name: llm-config

        # Storage
        volumeMounts:
        - name: storage
          mountPath: /storage

        resources:
          requests:
            cpu: "4"
            memory: "32Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "8"
            memory: "64Gi"
            nvidia.com/gpu: "1"

      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: llama-storage
