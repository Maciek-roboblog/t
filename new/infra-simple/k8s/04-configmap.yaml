# ConfigMap - konfiguracja ścieżek i parametrów
#
# ŚCIEŻKI MODELI:
# Wszystkie modele są NASZE WŁASNE (nie z HuggingFace)
# Przechowywane w PVC: llama-storage (NFS ReadWriteMany)
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-config
  namespace: llm-training
data:
  # ==========================================
  # ŚCIEŻKI W PVC (storage)
  # ==========================================

  # Model bazowy (wgrany wcześniej do PVC)
  BASE_MODEL_PATH: "/storage/models/base-model"

  # Katalog na adaptery LoRA (output treningu)
  LORA_OUTPUT_PATH: "/storage/output/lora-adapter"

  # Model po merge (input dla vLLM)
  MERGED_MODEL_PATH: "/storage/models/merged-model"

  # Katalog na datasety
  DATASET_PATH: "/storage/data"

  # ==========================================
  # KONFIGURACJA TRENINGU (LLaMA-Factory)
  # ==========================================

  FINETUNING_TYPE: "lora"
  LORA_RANK: "8"
  LORA_ALPHA: "16"
  TEMPLATE: "llama3"
  CUTOFF_LEN: "2048"

  # Hiperparametry
  BATCH_SIZE: "1"
  GRADIENT_ACCUMULATION: "8"
  LEARNING_RATE: "1.0e-4"
  NUM_EPOCHS: "3"

  # ==========================================
  # KONFIGURACJA INFERENCE (vLLM)
  # ==========================================

  # Nazwa modelu w API
  SERVED_MODEL_NAME: "llama-finetuned"

  # Maksymalna długość kontekstu
  MAX_MODEL_LEN: "4096"

  # Tensor parallelism (1 = single GPU)
  TENSOR_PARALLEL_SIZE: "1"
