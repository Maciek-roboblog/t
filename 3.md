
# Architektura platformy fine-tuningu i inferencji LLM z LLaMA-Factory (C4)

## Kontekst systemu (C4 – Poziom 1)

Platforma umożliwia zespołom Data Science trenowanie i udostępnianie własnych modeli **Large Language Model (LLM)** przy użyciu narzędzia **LLaMA-Factory** w chmurze (np. Google Cloud).

System obsługuje dwa główne przypadki użycia:

1. **Fine-tuning modeli LLM** na danych użytkownika,
2. **Inference** – udostępnianie wytrenowanych modeli jako usługi API do wykorzystania w aplikacjach.

Główni użytkownicy systemu:

- **Inżynierowie ML / Data Scientists** – konfigurują i uruchamiają procesy trenowania,
- **Klienci / aplikacje** – korzystają z wdrożonych modeli poprzez API.

W rozwiązaniu uwzględniono:

- wymagania **multi-tenant** (współdzielenie platformy przez wiele zespołów),
- zgodność z **AI Act** (kategoryzacja modeli, dokumentacja danych i modeli, rejestrowanie logów).

---

## Architektura fine-tuningu (wersja A)

Wariant A skupia się na pipeline **fine-tuningu modelu** (trening dostosowawczy).

### Diagram (C4 – poziom kontenerów)

```plantuml
@startuml
!define ICONURL https://raw.githubusercontent.com/tupadr3/plantuml-icon-fontsprites/v2.1.0
!include ICONURL/common.puml
!include ICONURL/DevOps/helm.puml
!include ICONURL/AWS/cloud.puml // using cloud icon for GCP for simplicity

skinparam wrapWidth 200
skinparam defaultTextAlignment center

' Definicja legendy dla oznaczeń AI Act
skinparam rectangle<<AIAct>> {
  BackgroundColor #FFF2E5
  BorderColor #FF8040
  BorderThickness 2
}

actor "Data Scientist\n(zespół ML)" as Scientist <<Human>>

rectangle "Projekt GCP" <<cloud>> {
  node "Klaster Kubernetes" as Cluster {
    package "Argo Workflows\n(Orkiestracja Pipeline)" as Argo <<framework>> {
      node "Argo UI/CLI" as ArgoUI <<framework>>
      node "Kontroler Argo\n(Workflow Controller)" as ArgoCtrl <<framework>>
    }

    frame "Namespace Team A" as NSA {
      component "Pipeline treningowy\n(Workflow)" as Workflow
      component "Zadanie: Fine-tuning LLM\n(Pod z LLaMA-Factory)" as TrainPod <<container>>
      component "Zadanie: Ewaluacja\n(Pod z LLaMA-Factory)" as EvalPod <<container>>
    }

    frame "Namespace Team B" as NSB {
      note "Inny zespół (multi-tenant)" as N2
    }

    node "Storage artefaktów\n(np. GCS bucket)" as ArtifactStore <<AIAct>>
    database "Repozytorium konfiguracji\n(Git)" as GitRepo
  }
}

Scientist -> ArgoUI : definiuje eksperyment\n(YAML config, dane)
ArgoUI -> ArgoCtrl : zgłasza uruchomienie\npipeline
ArgoCtrl -> Workflow : inicjuje Workflow (CRD)
Workflow -> TrainPod : [krok] Fine-tuning (trening modelu)
TrainPod -> ArtifactStore : zapisuje wytrenowany model\n(checkpointy, finalny model)
Workflow -> EvalPod : [krok] Ewaluacja/wersja\nmodelu (opcjonalnie)
EvalPod -> ArtifactStore : zapisuje metryki, model card\n(opis modelu)
Workflow --> ArgoUI : status postępu/ wyniki
@enduml
````

### Opis komponentów (fine-tuning)

* **Data Scientist (użytkownik)**
  Inicjuje proces trenowania poprzez UI lub CLI. Przygotowuje konfigurację trenowania (model bazowy, hiperparametry, dane treningowe) i przekazuje ją do systemu.

* **UI/CLI (Argo UI lub LLaMA-Factory WebUI)**
  Interfejs, przez który użytkownik uruchamia pipeline:

  * Argo Web UI / API,
  * LLaMA-Factory **LLaMA Board** – WebUI do zarządzania eksperymentami; w chmurze integruje się z Argo.

* **Argo Workflows**
  Silnik orkiestracji zadań na Kubernetes.
  Składa się z:

  * kontrolera w klastrze K8s,
  * opcjonalnego API/UI.
    Pipeline zdefiniowany w YAML zawiera kroki: przygotowanie danych, fine-tuning, ewaluacja. Argo uruchamia każdy krok jako kontener.

* **LLaMA-Factory (TrainPod/EvalPod)**
  Kontener z frameworkiem LLaMA-Factory, uruchamiany w krokach pipeline:

  * obsługuje fine-tuning 100+ modeli LLM,
  * integruje Accelerate, DeepSpeed, Bitsandbytes itp.,
  * w trybie treningowym pobiera model bazowy i dane, wykonuje trening (np. LoRA/QLoRA),
  * może uruchomić ewaluację,
  * wykorzystuje GPU w klastrze (np. GKE node pool z Nvidia L4).

* **Repozytorium konfiguracji (Git)**
  Centralne repo (GitHub/GitLab) z:

  * definicjami pipeline’ów,
  * parametrami eksperymentów,
  * konfiguracjami modeli.
    Służy także jako źródło GitOps (np. Argo CD aktualizuje workflowy na klastrze).

* **Storage artefaktów**
  Magazyn wyników trenowania:

  * wytrenowane modele,
  * checkpointy,
  * logi,
  * model cards.
    Typowo: **GCS bucket** na GCP. Kubernetes może montować go jako wolumen (GCS Fuse CSI). W wersji podstawowej pełni rolę model registry.

* **Workflow i zależności**
  Pipeline trenowania działa w izolowanym środowisku (Namespace danego zespołu).
  Argo tworzy obiekty `Workflow` i `Pod` w tej przestrzeni nazw, co wspiera **logical multi-tenancy**.

### Przepływ działania (fine-tuning)

1. Inżynier ML przygotowuje definicję eksperymentu (YAML z parametrami modelu, ścieżkami do danych, konfiguracją LoRA) i:

   * commituje ją do repozytorium konfiguracji **lub**
   * wprowadza przez UI.
2. Argo Workflows pobiera definicję i uruchamia **Workflow** – sekwencję zadań.
3. Wykonywane są kroki przygotowawcze (np. załadowanie datasetu z GCS, preprocessing).
4. Uruchamiany jest kontener LLaMA-Factory do **fine-tuningu**:

   * loguje metryki (W&B, TensorBoard, itp.),
   * po zakończeniu zapisuje model i statystyki do storage artefaktów.
5. Opcjonalny krok **ewaluacji**:

   * generuje odpowiedzi na zbiorze walidacyjnym,
   * liczy metryki jakości,
   * generuje **model card**.
6. Użytkownik śledzi postęp i wyniki:

   * przez Argo UI (status kroków, logi),
   * lub dashboard eksperymentów (np. LLaMA Board).
7. Po zakończeniu pipeline zespół ma:

   * gotowy artefakt modelu (`.pt`, `safetensors`, itp.),
   * powiązaną dokumentację (metryki, karta modelu).

---

## Architektura rozszerzona o inference (wersja B)

Wariant B rozszerza środowisko o komponenty potrzebne do **udostępniania modelu jako usługi (inferencja)**:

* deployment modelu jako API,
* autoryzacja dostępu,
* monitoring i logging wykorzystania,
* routing ruchu do instancji modeli.

### Diagram (fine-tuning + inference)

```plantuml
@startuml
!define ICONURL https://raw.githubusercontent.com/tupadr3/plantuml-icon-fontsprites/v2.1.0
!include ICONURL/common.puml
!include ICONURL/DevOps/prometheus.puml

skinparam wrapWidth 200
skinparam defaultTextAlignment center

skinparam rectangle<<AIAct>> {
  BackgroundColor #FFF2E5
  BorderColor #FF8040
  BorderThickness 2
}

skinparam component<<new>> {
  BackgroundColor #D9E8FB
}

actor "Użytkownik\nAPI klient" as APIclient <<Human>>

rectangle "Klaster Kubernetes (Inferencja)" as InfCluster {
  component "Inference Service\n(Model LLM jako API)" as ModelAPI <<new>> {
    [Model Pod\n(LLaMA-Factory API/vLLM)] as ModelPod
  }
  component "API Gateway\n(Autoryzacja, Routing)" as APIGW <<new>>
  queue "Load Balancer\n/ Istio Ingress" as LB <<new>>
  component "Monitoring\nPrometheus, Grafana" as Monitoring <<new>><<AIAct>>
  component "Logi i audyty\n(Loki / GCP Logging)" as Logging <<AIAct>>
}

database "Registry modeli\n(metadane, kategorie)" as ModelRegistry <<AIAct>><<new>>
database "Karta modelu\n(Model Card repo)" as ModelCardRepo <<AIAct>><<new>>
storage "Artefakty modeli\n(GCS bucket)" as ModelStorage <<AIAct>>

Scientist2 <<Human>> as Scientist

Scientist --> ModelRegistry : rejestracja modelu\npo fine-tuningu
ModelAPI --> ModelStorage : ładuje wytrenowany\nmodel (wagę)
APIGW -> ModelAPI : zapytanie inferencyjne\n(tekst -> odpowiedź)
APIGW <- APIclient : zapytanie (prompt)
APIGW --> Monitoring : metryki (użycie, latencja)
ModelAPI -> Monitoring : metryki modelu (GPU, opóźnienia)
APIGW -> Logging : log żądania (kto, kiedy, parametry)
ModelAPI -> Logging : log szczegółowy (prompt, wynik)
@enduml
```

### Nowe komponenty (inference)

* **Registry modeli (Model Registry)**
  Wewnętrzna baza metadanych o wytrenowanych modelach. Po fine-tuningu model jest tu rejestrowany (ręcznie lub automatycznie):

  * ID/wersja modelu,
  * lokalizacja artefaktów (GCS, Artifactory),
  * parametry treningu, autorzy,
  * kategoria ryzyka AI Act (high-risk vs general-purpose),
  * link do model card.
    Spełnia wymogi rejestru systemów AI (audyt, zgodność).

* **Karta modelu (Model Card)**
  Ustrukturyzowany dokument opisujący:

  * przeznaczenie,
  * ograniczenia,
  * wyniki testów,
  * potencjalne biasy.
    Tworzona wg zaleceń „Model Cards”, częściowo automatycznie po treningu, uzupełniana przez zespół.
    Przechowywana w repozytorium (Markdown/HTML, baza, system dokumentów).
    Wymagana dla modeli wysokiego ryzyka (transparentność).

* **Inference Service (Model API)**
  Serwer odpowiedzialny za ładowanie modelu i obsługę zapytań:

  * może używać `llamafactory-cli api ...` z backendem **vLLM**,
  * udostępnia API w stylu OpenAI (`/v1/chat/completions` itp.),
  * wdrożenie jako `Deployment` lub `StatefulSet`.
    Przy starcie:
  * pobiera model z **ModelStorage** (bucket GCS),
  * ładuje go do GPU/CPU,
  * obsługuje zapytania inferencyjne.

* **API Gateway**
  Warstwa pośrednicząca:

  * **autentykacja i autoryzacja** (API key, OAuth, JWT, certyfikaty),
  * zapewnienie dostępu tylko dla uprawnionych użytkowników/tenantów,
  * routing – na podstawie ścieżki/hosta kieruje do właściwego modelu.
    Może być realizowany przez:
  * Istio Gateway / Envoy,
  * Cloud Endpoints / Apigee,
  * inny gateway.

* **Load Balancer / Ingress**
  Odpowiada za:

  * rozdzielanie ruchu między replikami serwisów modeli,
  * HA i skalowanie.
    Może być:
  * `Service type LoadBalancer`,
  * element Istio / Ingress Controller.

* **Monitoring (Prometheus, Grafana)**
  Zbiera metryki z:

  * API Gateway (liczba żądań, statusy),
  * kontenerów modeli (użycie GPU, czasy odpowiedzi).
    Grafana:
  * wizualizuje dashboardy,
  * wspiera alerty (np. spadek wydajności, błędy).
    Monitoring jest kluczowy:
  * dla rozliczeń (zużycie zasobów),
  * dla wymogów AI Act (ciągły nadzór nad działaniem modeli).

* **Logging (Loki / GCP Logging)**
  Centralny system logów i audytu:

  * loguje każde żądanie (kto, kiedy, jaki model),
  * loguje szczegóły (prompt, odpowiedź – z zachowaniem prywatności),
  * przechowuje logi min. 6 miesięcy (w praktyce dłużej).
    Pozwala:
  * odtworzyć działania systemu w razie incydentu,
  * spełnić wymogi **record-keeping** z AI Act,
  * budować alerty (np. wykrywanie niedozwolonych treści).

### Przepływ (inferencja)

1. Po fine-tuningu i rejestracji w **Model Registry**, zespół uruchamia usługę inference.
2. Argo Workflows / Argo CD wdraża `Deployment` dla modelu:

   * obraz LLaMA-Factory w trybie API,
   * konfiguracja ścieżki do modelu w GCS.
3. Po starcie serwisu model:

   * ładuje wagi z **ModelStorage**,
   * zgłasza gotowość.
4. Klient (aplikacja) wysyła zapytanie HTTP (REST/gRPC) do **API Gateway**.
5. Gateway:

   * weryfikuje uwierzytelnienie,
   * na podstawie URI/hosta kieruje żądanie do konkretnego modelu.
6. **Model API** generuje odpowiedź i odsyła ją przez Gateway do klienta.
7. Równolegle:

   * Prometheus zbiera metryki,
   * logi żądania i odpowiedzi trafiają do **Loki / GCP Logging**.
8. Administratorzy:

   * monitorują metryki w Grafanie,
   * analizują logi (audyt, debug, compliance).

### Skalowanie i routing

* Przy rosnącym obciążeniu:

  * zwiększamy liczbę replik podów modelu (Deployment),
  * Load Balancer/Ingress rozdziela ruch między repliki.
* Dla wielu modeli / wersji:

  * Gateway routuje po subdomenach/ścieżkach,
  * np. `model-a.team1.example.com` → model A zespołu 1,
  * `model-b.team2.example.com` → model B zespołu 2.
* Przy aktualizacji modelu:

  * Argo CD może zastosować **blue-green deployment**,
  * Gateway przełącza ruch po potwierdzeniu poprawności nowej wersji.

---

## Warianty multitenancy i ich wpływ

Platforma obsługuje wiele zespołów / klientów jednocześnie (**multi-tenancy**).

Rozważono dwa podejścia:

* **a) Logical tenancy** – wspólny klaster, logiczna izolacja,
* **b) Hard tenancy** – izolacja na poziomie infrastruktury.

### a) Logical tenancy – wspólny klaster, logiczna izolacja

Wszystkie zespoły korzystają ze wspólnego klastra Kubernetes i wspólnej infrastruktury, ale są odseparowane logicznie.

Mechanizmy:

* **Namespaces per zespół**

  * każdy zespół ma własny namespace,
  * w nim działają jego workflowy, pody treningowe i serwisy modeli.

* **RBAC i quotas**

  * restrykcyjne polityki **RBAC** – dostęp tylko do własnego namespace,
  * `ResourceQuota`/`LimitRange` per namespace:

    * np. limit 4 GPU, 100 CPU dla zespołu A,
    * zapobieganie efektowi „noisy neighbor”.

* **Wspólne usługi, współdzielone koszty**

  * Argo Workflows, Argo CD, monitoring/logi – wspólne, centralnie zarządzane,
  * niższy koszt – jedna instalacja Argo/Prometheus/Loki obsługuje wiele zespołów,
  * lepsze wykorzystanie zasobów (mniejsze „dziury” w obciążeniu).

* **Bezpieczeństwo**

  * logiczna izolacja jest słabsza niż fizyczna,
  * wymagane:

    * poprawne RBAC,
    * **Network Policies** izolujące ruch między namespace’ami,
    * polityki bezpieczeństwa (PSP/OPA Gatekeeper),
    * osobne buckety GCS per zespół (IAM).

* **Zarządzanie**

  * jeden klaster do administrowania,
  * scentralizowane upgrade’y (K8s, Argo, monitoring),
  * zmiany trzeba planować ostrożnie (dotykają wielu zespołów naraz).

**Podsumowanie:**
Logical tenancy obniża koszty, ułatwia dzielenie się wynikami (modele mogą być współużywane w klastrze) i upraszcza zarządzanie. Wymaga jednak dobrej konfiguracji bezpieczeństwa.

### b) Hard tenancy – izolacja na poziomie infrastruktury

Każdy tenant (zespół/klient SaaS) dostaje **wydzieloną infrastrukturę**.

Mechanizmy:

* **Oddzielne klastry K8s**

  * osobne klastry GKE/projekty GCP,
  * mocna izolacja – brak wspólnych punktów styku,
  * błąd w jednym klastrze nie wpływa na inne.

* **Oddzielne komponenty**

  * każdy klaster ma:

    * własne Argo Workflows / Argo CD,
    * własny stack monitoring/logging, itp.,
  * możliwe centralne zarządzanie konfiguracją (GitOps multi-cluster).

* **Bezpieczeństwo**

  * najwyższy poziom izolacji,
  * osobne IAM per projekt/klaster,
  * idealne dla:

    * zewnętrznych klientów (SaaS),
    * sektorów wrażliwych (finanse, medycyna),
    * scenariuszy z bardzo niskim zaufaniem między tenantami.

* **Koszt i wykorzystanie zasobów**

  * wyższy koszt i niższa efektywność:

    * powielone instancje (3 zespoły = 3x Argo, 3x Prometheus, itd.),
    * trudniej współdzielić chwilowo wolne GPU między tenantami.
  * multi-tenant w jednym klastrze bywa tańszy (lepsze wykorzystanie zasobów).

* **Zarządzanie**

  * większa złożoność operacyjna:

    * wiele klastrów do utrzymania,
    * monitoring federowany (Prometheus, Loki),
    * powtarzalne upgrade’y (automatyzacja konieczna).
  * pomocne:

    * Argo CD multi-cluster,
    * GKE fleet / Anthos,
    * vCluster.

**Podsumowanie:**
Hard tenancy zapewnia najlepszą izolację i jest preferowane w przypadku krytycznych wymagań bezpieczeństwa i zgodności (np. dla zewnętrznych klientów). Kosztem jest wyższa złożoność i koszt operacyjny. Typowo:

* **logical tenancy** dla zespołów wewnętrznych,
* **hard tenancy** dla produkcyjnych instancji zewnętrznych.

---

## Wymagania prawne (AI Act) – zgodność i oznaczenia

AI Act wprowadza wymogi prawne, szczególnie dla systemów **wysokiego ryzyka**. Architektura uwzględnia je poprzez komponenty oznaczone <<AIAct>>.

Kluczowe aspekty:

* **Klasyfikacja modeli (risk classification)**
  Każdy model:

  * klasyfikowany jako **High-Risk AI** lub **General-Purpose AI**,
  * informacja przechowywana w **Model Registry** i dokumentacji.
    Dla high-risk:
  * dodatkowe procedury oceny zgodności,
  * rozszerzone logowanie,
  * obowiązkowa karta modelu,
  * dodatkowe kroki pipeline (testy, walidacja biasów).

* **Rejestr modeli (Model Registry)**
  Wewnętrzny katalog AI:

  * spełnia wymogi dokumentacji technicznej i rejestru systemów,
  * przechowuje:

    * kto/kiedy utworzył model,
    * wersję, parametry,
    * wyniki oceny,
    * kategorię ryzyka,
    * odwołania do datasetów i dokumentacji.

* **Dokumentacja datasetów (data provenance)**
  AI Act wymaga:

  * dokumentowania źródeł danych,
  * charakterystyki, czyszczenia, anotacji,
  * znanych biasów.
    W pipeline:
  * krok przygotowania danych wymusza opis zbioru (README/meta),
  * opis przechowywany z artefaktami lub w repo danych,
  * możliwa integracja z DVC/Git LFS (reproducibility).

* **Automatyczne logowanie i audyt**
  Komponent **Logging/Audyt**:

  * loguje każdą interakcję z modelem (kto, co, kiedy),
  * dla high-risk – wszystkie decyzje/predykcje istotne prawnie,
  * logi przechowywane min. 6 miesięcy (często dłużej),
  * logowane są też działania operacyjne (deployment, zmiany konfiguracji).
    Umożliwia:
  * przegląd logów dla audytora,
  * generowanie raportów.

* **Kontrola dostępu i bezpieczeństwo**
  Mechanizmy:

  * RBAC,
  * izolacja danych,
  * uwierzytelnienie API,
    wspierają wymogi:
  * ochrony danych,
  * kontroli dostępu,
  * human oversight i odpowiedzialności.

* **Publikacja karty modelu**
  Dla każdego wdrożonego modelu (szczególnie high-risk):

  * generowana jest **Model Card** (zbliżona do „System Card” z AI Act),
  * umieszczana w **ModelCardRepo**,
  * może być:

    * publikowana na portalu firmy (modele publiczne),
    * przekazywana klientom (modele na zamówienie).

* **Monitoring zgodności**
  Poprzez Monitoring + alerty:

  * ciągły nadzór nad działaniem modeli,
  * wykrywanie spadku wydajności,
  * wykrywanie niepożądanych zachowań,
  * możliwość „post-market monitoring” (wymóg AI Act).

Komponenty kluczowe z perspektywy regulacji (oznaczone <<AIAct>>):

* Model Registry,
* Model Card Repo,
* Storage artefaktów (dane i modele),
* Logging/Audyt,
* Monitoring.

Zespół MLOps musi zapewnić:

* odpowiednie zabezpieczenia (dostępy do logów, integralność rejestru modeli),
* spełnienie wymogów retencji danych i raportowania.

---

## Helm charts – wykorzystywane repozytoria

Wdrożenie platformy korzysta z gotowych Helm chartów i/lub obrazów.

* **LLaMA-Factory**

  * brak oficjalnego Helm charta,
  * repozytorium udostępnia **Dockerfile** do budowy obrazu z funkcjami trenowania i serwowania,
  * rekomendacja:

    * zbudować własny obraz (np. GCP Cloud Build),
    * umieścić w Artifact Registry,
    * używać w Deploymentach (trening + API).

* **Argo Workflows & Argo CD**

  * oficjalne charty w repo: `argo-helm` (Argo Project),
  * instalacja przykładowo:

    ```bash
    helm repo add argo https://argoproj.github.io/argo-helm
    helm install argo-workflows argo/argo-workflows
    ```
  * charty zawierają CRD Argo (Workflows, WorkflowTemplate, itp.) i konfigurację HA, RBAC, itp.

* **GCS jako storage (CSI driver)**

  * na GKE:

    * włączenie dodatku **GCS Fuse CSI Driver** (`--addons GcsFuseCsiDriver`),
    * automatyczna instalacja drivera,
  * dla innych K8s:

    * CSI driver for GCS (np. projekt `csi-gcs`),
    * dostępne helm charty społeczności (np. na ArtifactHub).
  * Alternatywnie:

    * usługi korzystają z API GCS poprzez SDK (bez CSI),
    * CSI wygodny dla pipeline’ów traktujących dane jak lokalny FS.

* **Monitoring – Prometheus, Grafana**

  * rekomendowany pakiet: `kube-prometheus-stack` (repo `prometheus-community`):

    * Prometheus,
    * Grafana,
    * Alertmanager,
    * domyślne dashboardy K8s.
  * Alternatywnie:

    * `prometheus-community/prometheus`,
    * `grafana/grafana` (repo Grafany).

* **Logging – Loki**

  * oficjalny chart: `grafana/loki-stack`,
  * instalacja np.:

    ```bash
    helm repo add grafana https://grafana.github.io/helm-charts
    helm install loki grafana/loki-stack
    ```
  * instaluje:

    * Loki,
    * Promtail (agent logów),
    * opcjonalnie Grafana.
  * W produkcji:

    * podłączenie trwałego storage (PV),
    * konfiguracja `boltdb-shipper` + backend (np. GCS) dla trwałości logów.

---

## Źródła

* Distributed OpenSource LLM Fine-Tuning with LLaMA-Factory on GKE – Google Cloud / Medium
* LLaMA Factory – dokumentacja
* Bitnami Secure Images Helm chart for Argo Workflows – AWS Marketplace
* Installation – Argo Workflows (oficjalna dokumentacja)
* GitHub – argoproj/argo-helm
* GitHub – hiyouga/LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)
* Complying with the EU AI Act: What Teams Need to Know – Label Studio
* What is a Model Card Report? Your Guide to Responsible AI – FairNow
* Using Prometheus, Loki, and Grafana to monitor in Kubernetes – Medium
* Article 12 & 19 – EU Artificial Intelligence Act (record-keeping, logs)
* Kubernetes Multi-Tenancy – oficjalna dokumentacja K8s i artykuły (StackGenie, Spectro Cloud)

